{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaximusThomas/ML-Projects/blob/main/convnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a86001",
      "metadata": {
        "id": "79a86001"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ff7447e",
      "metadata": {
        "id": "0ff7447e"
      },
      "outputs": [],
      "source": [
        "def convolution(A_prev, filters, biases, n_s, n_p):\n",
        "    '''\n",
        "    Single convolutional layer of all filters, assuming ReLU activation\n",
        "\n",
        "    Args:\n",
        "        A_prev ((m, n_H, n_W, n_C)): matrix of activations\n",
        "        filters ((n_filters, n_f, n_f, n_C)): filters\n",
        "        biases (n_filters): bias term for each filter\n",
        "        n_s (int): convolution stride\n",
        "        n_p (int): amount of padding (assumes square symmetrical)\n",
        "\n",
        "    Returns:\n",
        "        A_next ((m, n_H_, n_W_, n_filters)): output matrix of activations\n",
        "        cache (dict): Python dictionary containing keys \"A_prev\", \"filters\", \"biases\", \"stride\", \"padding\"\n",
        "    '''\n",
        "\n",
        "    # Get filter shape and activation height and width\n",
        "    n_f, n_filters = filters.shape[1], filters.shape[0]\n",
        "    m, n_H, n_W = A_prev.shape[0], A_prev.shape[1], A_prev.shape[2]\n",
        "\n",
        "    # Calculate output height and width\n",
        "    n_H_ = int(np.floor((n_H + (2 * n_p) - n_f) / n_s)) + 1\n",
        "    n_W_ = int(np.floor((n_W + (2 * n_p) - n_f) / n_s)) + 1\n",
        "\n",
        "    # Pad the input activation matrix\n",
        "    A_prev_pad = np.pad(A_prev,\n",
        "                        pad_width=((0, 0), (n_p, n_p), (n_p, n_p), (0, 0)),\n",
        "                        mode='constant')\n",
        "\n",
        "    Z = np.zeros((m, n_H_, n_W_, n_filters))\n",
        "\n",
        "    # Cache required values\n",
        "    cache = {}\n",
        "    cache['A_prev'], cache['filters'], cache['biases'] = A_prev, filters, biases\n",
        "    cache['stride'], cache['padding'] = n_s, n_p\n",
        "\n",
        "    # Loop over all examples\n",
        "    for i in tqdm(range(m)):\n",
        "\n",
        "        # Loop over all filter positions\n",
        "        for h in range(n_H_):\n",
        "            for w in range(n_W_):\n",
        "\n",
        "                # Loop over all filters; returns filter of shape (n_f, n_f, n_C)\n",
        "                for filter_idx, (bias, filter) in enumerate(zip(biases, filters)):\n",
        "\n",
        "                    # Calculate indices for activation matrix slicing\n",
        "                    vert_start = h * n_s\n",
        "                    vert_end = vert_start + n_f\n",
        "                    horiz_start = w * n_s\n",
        "                    horiz_end = horiz_start + n_f\n",
        "\n",
        "                    # Element-wise multiplication and\n",
        "                    z = (A_prev_pad[i, vert_start:vert_end, horiz_start:horiz_end, :] *  filter) + bias\n",
        "                    z = np.sum(z)\n",
        "                    Z[i, h, w, filter_idx] = z\n",
        "\n",
        "\n",
        "    cache['Z'] = Z\n",
        "    # ReLU activation function\n",
        "    A_next = np.maximum(Z, 0)\n",
        "    cache['A_next'] = A_next\n",
        "\n",
        "    return A_next, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c024408e",
      "metadata": {
        "id": "c024408e"
      },
      "outputs": [],
      "source": [
        "def max_pool(A_prev, pool_size, n_s):\n",
        "    '''\n",
        "    Max pooling step across all examples and filters\n",
        "\n",
        "    Args:\n",
        "        A_prev ((m, n_H, n_W, n_C)): matrix of activations\n",
        "        pool_size (int): size of pool, assuming square\n",
        "        n_s (int): stride\n",
        "\n",
        "    Returns:\n",
        "        A_next ((m, n_H_, n_W_, n_filters)): pooled output matrix of activations\n",
        "        cache (dict): Python dictionary containing keys \"A_prev\", \"pool_size\", \"stride\"\n",
        "    '''\n",
        "\n",
        "    # Calculate output dimensions\n",
        "    m, n_H, n_W, n_C = A_prev.shape[0], A_prev.shape[1], A_prev.shape[2], A_prev.shape[3]\n",
        "    n_H_, n_W_ = int(np.floor((n_H - pool_size) / n_s)) + 1, int(np.floor((n_W - pool_size) / n_s)) + 1\n",
        "\n",
        "    A_next = np.zeros((m, n_H_, n_W_, n_C))\n",
        "    mask = np.zeros_like(A_prev)\n",
        "\n",
        "    # Cache values for backprop\n",
        "    cache = {}\n",
        "    cache['A_prev'], cache['pool_size'], cache['stride'] = A_prev, pool_size, n_s\n",
        "    # Loop over training examples\n",
        "    for i in range(m):\n",
        "\n",
        "        # Loop over all pooling positions\n",
        "        for h in range(int(n_H_)):\n",
        "            for w in range(int(n_W_)):\n",
        "\n",
        "                # Calculate indices for activation matrix slicing\n",
        "                vert_start = h * n_s\n",
        "                vert_end = vert_start + pool_size\n",
        "                horiz_start = w * n_s\n",
        "                horiz_end = horiz_start + pool_size\n",
        "\n",
        "                # Take np.max() across channels returning a vector and add into A_next\n",
        "                window = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "                A_next[i, h, w, :] = np.max(window, axis=(0, 1))\n",
        "\n",
        "                # Create mask for backpropagation\n",
        "                mask_slice = (window == np.max(window, axis=(0, 1), keepdims=True))\n",
        "                mask[i, vert_start:vert_end, horiz_start:horiz_end, :] = mask_slice\n",
        "\n",
        "    cache['mask'] = mask\n",
        "    return A_next, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32dcff97",
      "metadata": {
        "id": "32dcff97"
      },
      "outputs": [],
      "source": [
        "def flatten(A_prev):\n",
        "    '''\n",
        "    Flattens input matrix into 1D vector ready for a dense layer\n",
        "\n",
        "    Args:\n",
        "        A_prev ((m, n_H, n_W, n_C)): matrix of activations to be flattened\n",
        "\n",
        "    Returns:\n",
        "        A_next ((n, m)): output matrix ready for dense (fully connected) network\n",
        "    '''\n",
        "\n",
        "    # Flatten 4D input tensor into 2D matrix of dimensions (n, n_H * n_W * n_C)\n",
        "    m = A_prev.shape[0]\n",
        "    A_next = A_prev.transpose(1, 2, 3, 0).reshape(-1, m)\n",
        "\n",
        "    return A_next"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a07b70ff",
      "metadata": {
        "id": "a07b70ff"
      },
      "outputs": [],
      "source": [
        "def conv_back_prop(dA_next, cache):\n",
        "    '''\n",
        "    Back prop for a single convolution layer\n",
        "\n",
        "    Args:\n",
        "        dZ ((m, n_H_, n_W_, n_filters)): gradient of linear dot product output\n",
        "        cache (): cache of A_prev, filters (weights), stride, and padding\n",
        "    '''\n",
        "\n",
        "    # Obtain required variables from layer cache\n",
        "    Z_linear_output = cache['Z'] # Corrected: 'Z_prev' to 'Z'\n",
        "    A_prev = cache['A_prev']\n",
        "    filters = cache['filters'] # Renamed W to filters to avoid name collision\n",
        "    n_s = cache['stride']\n",
        "    n_p = cache['padding'] # Added padding from cache\n",
        "    A_next = cache['A_next']\n",
        "\n",
        "    # Get shapes\n",
        "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
        "    n_filters, n_f, _, _ = filters.shape\n",
        "\n",
        "    dZ = dA_next * (A_next > 0)\n",
        "    m_dZ, n_H_out, n_W_out, _ = dZ.shape\n",
        "\n",
        "    # Initialize gradients\n",
        "    db = np.zeros((n_filters))\n",
        "    dW = np.zeros_like(filters) # dW has shape (n_filters, n_f, n_f, n_C_prev)\n",
        "    dA_prev = np.zeros_like(A_prev) # dA_prev has shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "\n",
        "    # Pad dA_prev for gradient accumulation\n",
        "    dA_prev_pad = np.pad(dA_prev,\n",
        "                         pad_width=((0, 0), (n_p, n_p), (n_p, n_p), (0, 0)),\n",
        "                         mode='constant')\n",
        "\n",
        "    # Pad A_prev for calculations (this is the A_prev_pad used in forward pass)\n",
        "    A_prev_padded = np.pad(A_prev,\n",
        "                         pad_width=((0, 0), (n_p, n_p), (n_p, n_p), (0, 0)),\n",
        "                         mode='constant')\n",
        "\n",
        "    # Loop over all training examples\n",
        "    for i in range(m):\n",
        "        # Loop over output positions\n",
        "        for h_out in range(n_H_out):\n",
        "            for w_out in range(n_W_out):\n",
        "                vert_start = h_out * n_s\n",
        "                vert_end = vert_start + n_f\n",
        "                horiz_start = w_out * n_s\n",
        "                horiz_end = horiz_start + n_f\n",
        "\n",
        "                # Slice the relevant window from A_prev_padded\n",
        "                A_prev_window = A_prev_padded[i, vert_start:vert_end, horiz_start:horiz_end, :]\n",
        "\n",
        "                # Loop over all filters\n",
        "                for filter_idx in range(n_filters):\n",
        "                    current_filter = filters[filter_idx, :, :, :] # Shape (n_f, n_f, n_C_prev)\n",
        "\n",
        "                    # Accumulate gradient for A_prev_pad\n",
        "                    # dL/dA_prev_element += dL/dZ_element * W_element\n",
        "                    dA_prev_pad[i, vert_start:vert_end, horiz_start:horiz_end, :] += \\\n",
        "                        current_filter * dZ[i, h_out, w_out, filter_idx]\n",
        "\n",
        "                    # Accumulate gradient for W (filters)\n",
        "                    # dL/dW_element += A_prev_element * dL/dZ_element\n",
        "                    dW[filter_idx, :, :, :] += A_prev_window * dZ[i, h_out, w_out, filter_idx]\n",
        "\n",
        "                    # Accumulate gradient for b (bias)\n",
        "                    # dL/db_element += dL/dZ_element\n",
        "                    db[filter_idx] += dZ[i, h_out, w_out, filter_idx]\n",
        "\n",
        "    # Unpad dA_prev_pad to get dA_prev (gradient w.r.t. unpadded input to this layer)\n",
        "    if n_p > 0:\n",
        "        dA_prev = dA_prev_pad[:, n_p:-n_p, n_p:-n_p, :]\n",
        "    else:\n",
        "        dA_prev = dA_prev_pad\n",
        "\n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cd54080",
      "metadata": {
        "id": "1cd54080"
      },
      "outputs": [],
      "source": [
        "def max_pool_backprop(dA_next, A_prev, pool_size, n_s, mask):\n",
        "    '''\n",
        "    Backprop for a max pooling layer\n",
        "\n",
        "    Args:\n",
        "        dA_next ((m, n_H_, n_W_, n_filters)): tensor of activations from max pooling layer\n",
        "        pool_size (int): size of pool, assuming square\n",
        "        n_s (int): stride\n",
        "        mask ((m, n_H, n_W, n_C)): tensor of either 0s or 1s corresponding to indices of max values\n",
        "\n",
        "    Returns:\n",
        "        dA_prev ((m, n_H, n_W, n_C)): tensor of activations from previous conv layer\n",
        "    '''\n",
        "\n",
        "    (m, n_H_, n_W_, n_C_) = dA_next.shape\n",
        "    dA_prev = np.zeros_like(A_prev)\n",
        "\n",
        "    # Loop over training examples\n",
        "    for i in range(m):\n",
        "\n",
        "        # Loop over dimensions\n",
        "        for h in range(n_H_):\n",
        "            for w in range(n_W_):\n",
        "\n",
        "                # Get window positions\n",
        "                vert_start = h * n_s\n",
        "                vert_end = vert_start + pool_size\n",
        "                horiz_start = h * n_s\n",
        "                horiz_end = horiz_start + pool_size\n",
        "\n",
        "                dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, :] += mask[i, vert_start:vert_end, horiz_start:horiz_end, :] * dA_next[i, h, w, :]\n",
        "\n",
        "    return dA_prev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d6641d9",
      "metadata": {
        "id": "7d6641d9"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "\n",
        "    x = np.clip(x, -500, 500)\n",
        "    s = 1/(1+np.exp(-x))\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c6b8cf4",
      "metadata": {
        "id": "7c6b8cf4"
      },
      "outputs": [],
      "source": [
        "def binary_cross_entropy_cost(yhat, y, epsilon = 1e-15):\n",
        "\n",
        "    m = y.shape[1]\n",
        "    yhat = np.clip(yhat, epsilon, 1 - epsilon)\n",
        "    cost = -1/m * np.sum((y * np.log(yhat)) + ((1-y) * np.log(1-yhat)))\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8723fdb9",
      "metadata": {
        "id": "8723fdb9"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(X, dense_dims, conv_params, conv_dims):\n",
        "\n",
        "\n",
        "    m = X.shape[0]\n",
        "    A_prev = X\n",
        "    l = 1\n",
        "    for dim in conv_dims:\n",
        "\n",
        "      if dim == 'conv':\n",
        "\n",
        "        # Single convolutional layer with ReLU activation\n",
        "        A_next, _ = convolution(A_prev,\n",
        "                                    filters=conv_params['W' + str(l)],\n",
        "                                    biases=conv_params['b' + str(l)],\n",
        "                                    n_s=conv_dims[dim]['n_s'],\n",
        "                                    n_p=conv_dims[dim]['n_p'])\n",
        "        l += 1\n",
        "\n",
        "\n",
        "      elif dim == 'max_pool':\n",
        "        A_next, _ = max_pool(A_prev, conv_dims[dim]['pool_size'], conv_dims[dim]['n_s'])\n",
        "\n",
        "      A_prev = A_next\n",
        "\n",
        "    dense_dims.insert(0, A_prev.reshape(-1, m).shape[0])\n",
        "\n",
        "    parameters = {}\n",
        "    for l in range(1, len(dense_dims)):\n",
        "        parameters['W' + str(l)] = np.random.randn(dense_dims[l], dense_dims[l - 1]) * np.sqrt(2/dense_dims[l-1])\n",
        "        parameters['b' + str(l)] = np.zeros((dense_dims[l], 1))\n",
        "\n",
        "    print(f'Shape of W1: {parameters['W1'].shape}')\n",
        "\n",
        "    return parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5819464c",
      "metadata": {
        "id": "5819464c"
      },
      "outputs": [],
      "source": [
        "def forward_prop(conv_prev, y, parameters):\n",
        "    '''\n",
        "    Forward prop for dense layers, flattening convolutional output, assuming ReLU activation and sigmoid output activation\n",
        "\n",
        "    Args:\n",
        "        conv_prev ((m, n_H, n_W, n_filters)): tensor of activations from final max pooling layer\n",
        "        parameters (dict): dictionary containing all dense network parameters\n",
        "            W ((n_l, n_l_prev)): matrix of weights\n",
        "            b ((n_l, 1)): column vector of bias terms\n",
        "        y ((1, m)): labels\n",
        "\n",
        "    Returns:\n",
        "        yhat ((1, m)): predictions from forward prop\n",
        "    '''\n",
        "\n",
        "    # Flatten A_prev into a 2D matrix\n",
        "    m = conv_prev.shape[0]\n",
        "    A_prev = conv_prev.reshape(-1, m)\n",
        "\n",
        "    # Forward prop through dense network\n",
        "    caches = []\n",
        "    L = len(parameters) // 2\n",
        "    for l in range(1, L):\n",
        "        cache = {}\n",
        "\n",
        "        # Calculate linear product\n",
        "        Z = np.dot(parameters['W' + str(l)], A_prev) + parameters['b' + str(l)]\n",
        "        print(f'Shape of Z from layer {l}: {Z.shape}')\n",
        "        cache['Z' + str(l)] = Z\n",
        "        cache['W' + str(l)] = parameters['W' + str(l)]\n",
        "\n",
        "        # Apply activation function\n",
        "        A_next = np.max(Z, 0)\n",
        "        cache['A' + str(l-1)] = A_prev\n",
        "        A_prev = A_next\n",
        "        print(f'Shape of A_next from layer {l}: {A_next.shape}')\n",
        "\n",
        "        # Append cache\n",
        "        caches.append(cache)\n",
        "\n",
        "    # Output layer\n",
        "    cache = {}\n",
        "    cache['A' + str(L-1)] = A_prev\n",
        "    Z = np.dot(parameters['W' + str(L)], A_prev) + parameters['b' + str(L)]\n",
        "    cache['Z' + str(L)] = Z\n",
        "    cache['W' + str(L)] = parameters['W' + str(L)]\n",
        "    caches.append(cache)\n",
        "    yhat = sigmoid(A_next)\n",
        "\n",
        "    # Calculate loss\n",
        "    cost = binary_cross_entropy_cost(yhat, y)\n",
        "\n",
        "    return cost, yhat, caches\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc2415a6",
      "metadata": {
        "id": "cc2415a6"
      },
      "outputs": [],
      "source": [
        "def dense_back_prop(yhat, y, caches):\n",
        "    '''\n",
        "    Backprop for dense layers, assuming ReLU activation and sigmoid output activation\n",
        "\n",
        "    Args:\n",
        "        yhat ((1, m)): vector of activations of output layer for each example\n",
        "        y ((1, m)): vector of labels for each training example\n",
        "        caches (list): list of each layer's cache dictionary\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    '''\n",
        "\n",
        "    m = yhat.shape[1]\n",
        "    grads = {}\n",
        "    L = len(caches)\n",
        "\n",
        "    # Backprop for output layer\n",
        "    cache = caches[-1]\n",
        "    grads['dZ' + str(L)] = yhat - y\n",
        "    grads['dW' + str(L)] = 1/m * np.dot(grads['dZ' + str(L)], cache['A' + str(L-1)].T)\n",
        "    grads['db' + str(L)] = 1/m * np.sum(grads['dZ' + str(L)], axis=1, keepdims=True)\n",
        "    grads['dA' + str(L-1)] = np.dot(cache['W' + str(L)].T, grads['dZ' + str(L)])\n",
        "\n",
        "    # Backprop for hidden layers\n",
        "    for l in range(L-1, 0, -1):\n",
        "\n",
        "        cache = caches[l-1]\n",
        "        grads['dZ' + str(l)] = grads['dA' + str(l)] * (cache['Z' + str(l)] > 0)\n",
        "        grads['dW' + str(l)] = 1/m * np.dot(grads['dZ' + str(l)], cache['A' + str(l-1)].T)\n",
        "        grads['db' + str(l)] = 1/m * np.sum(grads['dZ' + str(l)], axis=1, keepdims=True)\n",
        "        grads['dA' + str(l-1)] = np.dot(cache['W' + str(l)].T, grads['dZ' + str(l)])\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c84f820f",
      "metadata": {
        "id": "c84f820f"
      },
      "outputs": [],
      "source": [
        "def initialize_convolution(X, conv_dims):\n",
        "    '''\n",
        "    Initializes parameters for convolutional part of network\n",
        "\n",
        "    Args:\n",
        "        X (((m, n_H, n_W, n_C)): input matrix\n",
        "        conv_dims (dict): dict containing either 'conv', or 'max_pool' corresponding to a sub-dict\n",
        "            conv (dict): dict with keys 'n_f', 'n_s', 'n_p', 'n_filters'\n",
        "                n_f (int): kernel/filter size\n",
        "                n_s (int): stride size\n",
        "                n_p (int): padding size\n",
        "                n_filters (int): number of filters\n",
        "            max_pool (dict): dict with keys 'pool_size', 'n_s'\n",
        "                pool_size (int): max pooling size\n",
        "                n_s (int): max pooling stride\n",
        "\n",
        "    Returns:\n",
        "        conv_params (dict): dict containg keys 'Wl' (filters of layer l), and 'bl' (biases of layer l)\n",
        "    '''\n",
        "\n",
        "    conv_params = {}\n",
        "    n_C = X.shape[-1]\n",
        "\n",
        "    # Loop over architecture specifications in list\n",
        "    for conv_dim in conv_dims:\n",
        "\n",
        "        l = 1\n",
        "        # Create params for only convolutional layers\n",
        "        if conv_dim == 'conv':\n",
        "            n_filters, n_f = conv_dims[conv_dim]['n_filters'], conv_dims[conv_dim]['n_f']\n",
        "            conv_params['W' + str(l)] = np.random.randn(n_filters, n_f, n_f, n_C) * np.sqrt(2 / (n_f * n_f * n_C))\n",
        "            conv_params['b' + str(l)] = np.zeros((n_filters, 1, 1))\n",
        "            n_C = n_filters\n",
        "            l += 1\n",
        "\n",
        "    return conv_params\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def full_forwardprop(X, y, conv_dims, dense_dims, params):\n",
        "    '''\n",
        "    Forward prop for both convolutional and dense sections of the network, assuming ReLU activation\n",
        "\n",
        "    Args:\n",
        "        X ((m, n_H, n_W, n_C)): input matrix\n",
        "        conv_dims (dict): dict containing either 'conv', or 'max_pool' corresponding to a sub-dict\n",
        "            conv (dict): dict with keys 'n_f', 'n_s', 'n_p', 'n_filters'\n",
        "                n_f (int): kernel/filter size\n",
        "                n_s (int): stride size\n",
        "                n_p (int): padding size\n",
        "                n_filters (int): number of filters\n",
        "            max_pool (dict): dict with keys 'pool_size', 'n_s'\n",
        "                pool_size (int): max pooling size\n",
        "                n_s (int): max pooling stride\n",
        "\n",
        "    Returns:\n",
        "        yhat ((1, m)): predictions from forward prop\n",
        "    '''\n",
        "    (conv_params, dense_params) = params\n",
        "    A_prev = X\n",
        "    conv_caches = []\n",
        "    l = 1\n",
        "\n",
        "    # Enumerate through both convolutional and max pooling layers\n",
        "    for dim in conv_dims:\n",
        "\n",
        "      if dim == 'conv':\n",
        "\n",
        "        # Single convolutional layer with ReLU activation\n",
        "        A_next, cache = convolution(A_prev,\n",
        "                                    filters=conv_params['W' + str(l)],\n",
        "                                    biases=conv_params['b' + str(l)],\n",
        "                                    n_s=conv_dims[dim]['n_s'],\n",
        "                                    n_p=conv_dims[dim]['n_p'])\n",
        "        l += 1\n",
        "\n",
        "\n",
        "      elif dim == 'max_pool':\n",
        "        A_next, cache = max_pool(A_prev, conv_dims[dim]['pool_size'], conv_dims[dim]['n_s'])\n",
        "\n",
        "      conv_caches.append(cache)\n",
        "      A_prev = A_next\n",
        "\n",
        "    # Forward prop through dense layers\n",
        "    cost, yhat, dense_caches = forward_prop(A_prev, y, dense_params)\n",
        "\n",
        "    return cost, yhat, dense_caches, conv_caches"
      ],
      "metadata": {
        "id": "Xc3qJMeS3Whq"
      },
      "id": "Xc3qJMeS3Whq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def full_backprop(yhat, y, dense_caches, conv_caches, conv_dims):\n",
        "    '''\n",
        "    Backprop for both the dense and convolutional sections of the nework\n",
        "\n",
        "    Args:\n",
        "        yhat ((1, m)): vector of activations of output layer for each example\n",
        "        y ((1, m)): vector of labels for each training example\n",
        "        caches (list): list of each layer's cache dictionary\n",
        "          cache (dict): Python dictionary containing\n",
        "            A_prev ((m, n_H, n_W, n_C)): previous activation matrices\n",
        "            filters ((n_filters, n_f, n_f, n_C)): filters\n",
        "            biases (n_filters, 1): bias term for each filter\n",
        "            stride (int): stride size\n",
        "            padding (int): padding size\n",
        "        conv_dims (dict): dict containing either 'conv', or 'max_pool' corresponding to a sub-dict\n",
        "            conv (dict): dict with keys 'n_f', 'n_s', 'n_p', 'n_filters'\n",
        "                n_f (int): kernel/filter size\n",
        "                n_s (int): stride size\n",
        "                n_p (int): padding size\n",
        "                n_filters (int): number of filters\n",
        "            max_pool (dict): dict with keys 'pool_size', 'n_s'\n",
        "                pool_size (int): max pooling size\n",
        "                n_s (int): max pooling stride\n",
        "\n",
        "    Returns:\n",
        "        dense_grads (dict): dict containg keys 'W + str(l)' and 'b + str(l)\n",
        "        conv_grads (dict): dict containing keys 'W + str(l)' and 'b + str(l)\n",
        "    '''\n",
        "\n",
        "    dense_grads = dense_back_prop(yhat, y, dense_caches)\n",
        "\n",
        "    # Get gradient for output of dense section\n",
        "    dA_next = dense_grads['dA_prev']\n",
        "\n",
        "    conv_grads = {}\n",
        "    for l, dim in reversed(enumerate(conv_dims)):\n",
        "\n",
        "      # Get current cache from caches\n",
        "      conv_cache = conv_caches[l]\n",
        "\n",
        "      # Backprop for convolutional layer\n",
        "      if dim == 'conv':\n",
        "        dA_prev, dW, db = conv_back_prop(dA_next, conv_cache)\n",
        "\n",
        "        # Update conv_grads diciontary\n",
        "        conv_grads['dW' + str(l)], conv_grads['db' + str(l)] = dW, db\n",
        "\n",
        "      # Backprop for max pooling layer\n",
        "      elif dim == 'max_pool':\n",
        "        A_prev, pool_size, n_s, mask = conv_cache['A_prev'], conv_cache['pool_size'], conv_cache['stride'], conv_cache['mask']\n",
        "        dA_prev = max_pool_backprop(dA_next, A_prev, pool_size, n_s, mask)\n",
        "\n",
        "      # Update dA_next\n",
        "      dA_next = dA_prev\n",
        "\n",
        "    # Return gradient dictionaries\n",
        "    return conv_grads, dense_grads"
      ],
      "metadata": {
        "id": "ry4Vr1FtxTAe"
      },
      "id": "ry4Vr1FtxTAe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Adam(params, grads, S, V, iter, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    '''\n",
        "    Adam optimization algorithm, combining both RMSprop and gradient descent with momentum, as well as bias correction\n",
        "\n",
        "    Args:\n",
        "      parameters (tuple): tuple of conv_params and dense_params dictionaries\n",
        "        conv_params (dict): dict containg keys 'Wl' (filters of layer l), and 'bl' (biases of layer l)\n",
        "        dense_params (dict): dict with keys 'Wl' (weights of layer l) and 'bl' (biases of layer l)\n",
        "      grads (tuple): tuple of conv_grads and dense_grads dicts\n",
        "        conv_grads (dict): dict with keys 'dWl' and 'dbl' (i.e., the partial derivative of each parameter)\n",
        "        dense_grads (dict): dict with keys 'dWl' and 'dbl' (i.e., the partial derivative of each parameter)\n",
        "      S (tuple): contains exponential moving average of squared gradients dictionaries of conv_S and dense_S\n",
        "      V (tuple): contains exponential moving average of gradients dictionaries of conv_V and dense_V\n",
        "\n",
        "    Returns:\n",
        "      parameters (tuple): updated tuple of conv_params and dense_params dictionaries\n",
        "        conv_params (dict): updated dict containg keys 'Wl' (filters of layer l), and 'bl' (biases of layer l)\n",
        "        dense_params (dict): updated dict with keys 'Wl' (weights of layer l) and 'bl' (biases of layer l)\n",
        "      S (tuple): contains exponential moving average of squared gradients dictionaries of conv_S and dense_S\n",
        "      V (tuple): contains exponential moving average of gradients dictionaries of conv_V and dense_V\n",
        "    '''\n",
        "\n",
        "    # Unpack tuples\n",
        "    (conv_params, dense_params) = params\n",
        "    (conv_grads, dense_grads) = grads\n",
        "    (conv_S, dense_S) = S\n",
        "    (conv_V, dense_V) = V\n",
        "\n",
        "    # Iterate over all dense_params\n",
        "    L = len(dense_params)//2\n",
        "    for l in range(1, L+1):\n",
        "\n",
        "        # Get gradients\n",
        "        dWl, dbl = dense_grads['dW'+str(l)], dense_grads['db'+str(l)]\n",
        "\n",
        "        # Update velocity terms\n",
        "        dense_V['dW'+str(l)] = (beta1 * dense_V['dW'+str(l)]) + ((1-beta1) * dWl)\n",
        "        dense_V['db'+str(l)] = (beta1 * dense_V['db'+str(l)]) + ((1-beta1) * dbl)\n",
        "\n",
        "        # Update squared-gradient terms\n",
        "        dense_S['dW' + str(l)] = (beta2 * dense_S['dW' + str(l)]) + ((1 - beta2) * np.square(dWl))\n",
        "        dense_S['db' + str(l)] = (beta2 * dense_S['db' + str(l)]) + ((1 - beta2) * np.square(dbl))\n",
        "\n",
        "        # Bias correction\n",
        "        vdW_corrected = dense_V['dW'+str(l)] / (1 - (beta1 ** iter))\n",
        "        vdb_corrected = dense_V['db'+str(l)] / (1 - (beta1 ** iter))\n",
        "\n",
        "        sdW_corrected = dense_S['dW' + str(l)] / (1 - (beta2 ** iter))\n",
        "        sdb_corrected = dense_S['db' + str(l)] / (1 - (beta2 ** iter))\n",
        "\n",
        "        # Update Parameters\n",
        "        dense_params['W'+str(l)] -= (alpha * vdW_corrected) / (np.sqrt(sdW_corrected)+epsilon)\n",
        "        dense_params['b'+str(l)] -= (alpha * vdb_corrected) / (np.sqrt(sdb_corrected)+epsilon)\n",
        "\n",
        "  # Iterate over all conv_params\n",
        "    L = len(conv_params)//2\n",
        "    for l in range(1, L+1):\n",
        "\n",
        "        # Get gradients\n",
        "        dWl, dbl = conv_grads['dW'+str(l)], conv_grads['db'+str(l)]\n",
        "\n",
        "        # Update velocity terms\n",
        "        conv_V['dW'+str(l)] = (beta1 * conv_V['dW'+str(l)]) + ((1-beta1) * dWl)\n",
        "        conv_V['db'+str(l)] = (beta1 * conv_V['db'+str(l)]) + ((1-beta1) * dbl)\n",
        "\n",
        "        # Update squared-gradient terms\n",
        "        conv_S['dW' + str(l)] = (beta2 * conv_S['dW' + str(l)]) + ((1 - beta2) * np.square(dWl))\n",
        "        conv_S['db' + str(l)] = (beta2 * conv_S['db' + str(l)]) + ((1 - beta2) * np.square(dbl))\n",
        "\n",
        "        # Bias correction\n",
        "        vdW_corrected = conv_V['dW'+str(l)] / (1 - (beta1 ** iter))\n",
        "        vdb_corrected = conv_V['db'+str(l)] / (1 - (beta1 ** iter))\n",
        "\n",
        "        sdW_corrected = conv_S['dW' + str(l)] / (1 - (beta2 ** iter))\n",
        "        sdb_corrected = conv_S['db' + str(l)] / (1 - (beta2 ** iter))\n",
        "\n",
        "        # Update Parameters\n",
        "        conv_params['W'+str(l)] -= (alpha * vdW_corrected) / (np.sqrt(sdW_corrected)+epsilon)\n",
        "        conv_params['b'+str(l)] -= (alpha * vdb_corrected) / (np.sqrt(sdb_corrected)+epsilon)\n",
        "\n",
        "    # Repack tuples\n",
        "    params = (conv_params, dense_params)\n",
        "    S = (conv_S, dense_S)\n",
        "    V = (conv_V, dense_V)\n",
        "\n",
        "    return params, S, V"
      ],
      "metadata": {
        "id": "gsrck3Sws9iv"
      },
      "id": "gsrck3Sws9iv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_adam(params):\n",
        "\n",
        "  # Initialize adam\n",
        "  (conv_params, dense_params) = params\n",
        "  dense_S, conv_S, dense_V, conv_V = {}, {}, {}, {}\n",
        "  S, V = (conv_S, dense_S), (conv_V, dense_V)\n",
        "\n",
        "  # Loop over dense params\n",
        "  L = len(dense_params)//2\n",
        "  for l in range(1, L+1):\n",
        "\n",
        "    # RMS prop terms\n",
        "    dense_S['dW'+str(l)] = np.zeros_like(dense_params['W'+str(l)])\n",
        "    dense_S['db'+str(l)] = np.zeros_like(dense_params['b'+str(l)])\n",
        "\n",
        "    # Momentum terms\n",
        "    dense_V['dW'+str(l)] = np.zeros_like(dense_params['W'+str(l)])\n",
        "    dense_V['db'+str(l)] = np.zeros_like(dense_params['b'+str(l)])\n",
        "\n",
        "  L = len(conv_params)//2\n",
        "  for l in range(1, L+1):\n",
        "    # RMS prop terms\n",
        "    conv_S['dW'+str(l)] = np.zeros_like(conv_params['W'+str(l)])\n",
        "    conv_S['db'+str(l)] = np.zeros_like(conv_params['b'+str(l)])\n",
        "\n",
        "    # Momentum terms\n",
        "    conv_V['dW'+str(l)] = np.zeros_like(conv_params['W'+str(l)])\n",
        "    conv_V['db'+str(l)] = np.zeros_like(conv_params['b'+str(l)])\n",
        "\n",
        "  S, V = (conv_S, dense_S), (conv_V, dense_V)\n",
        "\n",
        "  return S, V\n",
        "\n"
      ],
      "metadata": {
        "id": "0xL_V05r0xNJ"
      },
      "id": "0xL_V05r0xNJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X, y, dims, epochs=10):\n",
        "    '''\n",
        "    Forward and backprop to train CNN model\n",
        "\n",
        "    Args:\n",
        "        X ((m, n_H, n_W, n_C)): inputs\n",
        "    '''\n",
        "    # Extract network architecture\n",
        "    (conv_dims, dense_dims) = dims\n",
        "\n",
        "    # Initialize parameters\n",
        "    conv_params = initialize_convolution(X, conv_dims)\n",
        "    dense_params = initialize_parameters(X, dense_dims, conv_params, conv_dims)\n",
        "    params = (conv_params, dense_params)\n",
        "\n",
        "    # Empty list to record cost\n",
        "    cost_history = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(1, epochs+1)):\n",
        "\n",
        "      # Full forward prop\n",
        "      cost, yhat, dense_caches, conv_caches = full_forwardprop(X, y, conv_dims, dense_dims, params)\n",
        "      cost_history.append(cost)\n",
        "\n",
        "      # Full backprop\n",
        "      conv_grads, dense_grads = full_backprop(yhat, y, dense_caches, conv_caches)\n",
        "      grads = (conv_grads, dense_grads)\n",
        "\n",
        "      # Initialize Adam\n",
        "      S, V = initialize_adam(params)\n",
        "\n",
        "      # Adam optimization algorithm\n",
        "      params, S, V = Adam(params, grads, S, V, epoch)\n",
        "\n",
        "    return params, cost_history"
      ],
      "metadata": {
        "id": "a_aDEG4mypSg"
      },
      "id": "a_aDEG4mypSg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load data\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "\n",
        "# Select classes 0 and 1 for binary classification\n",
        "mask = (train_y == 0) | (train_y == 1)\n",
        "train_X, train_y = train_X[mask.flatten()], train_y[mask]\n",
        "\n",
        "# Reshape X to (m, n_H, n_W, n_C)\n",
        "train_X = train_X.reshape(train_X.shape[0], 28, 28, 1)\n",
        "test_X = test_X.reshape(test_X.shape[0], 28, 28, 1)\n",
        "\n",
        "# Reshape y to (1, m)\n",
        "train_y = train_y.reshape(1, train_y.shape[0])\n",
        "test_y = test_y.reshape(1, test_y.shape[0])\n",
        "\n",
        "# Normalization\n",
        "train_X = train_X / 255.0\n",
        "test_X = test_X / 255.0"
      ],
      "metadata": {
        "id": "XADEVct3wUgy"
      },
      "id": "XADEVct3wUgy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape"
      ],
      "metadata": {
        "id": "w1GzzqClHtB3"
      },
      "id": "w1GzzqClHtB3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify conv net architecture\n",
        "conv_1 = {'n_f' : 3, 'n_s' : 1, 'n_p' : 0, 'n_filters' : 8}\n",
        "pool_1 = {'pool_size' : 2, 'n_s' : 2}\n",
        "conv_dims = {'conv' : conv_1, 'max_pool' : pool_1}\n",
        "\n",
        "# Specify dense network architecuture\n",
        "dense_dims = [64, 1]\n",
        "\n",
        "# Overall network architecture\n",
        "dims = (conv_dims, dense_dims)\n",
        "\n",
        "# Train network\n",
        "params, cost_history = train(train_X[:100, :, :, :], train_y, dims, epochs=5)"
      ],
      "metadata": {
        "id": "njacxGZ731pl"
      },
      "id": "njacxGZ731pl",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}