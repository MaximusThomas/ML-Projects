{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaximusThomas/ML-Projects/blob/main/convnet_multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XCgpaUX6H7cd",
      "metadata": {
        "id": "XCgpaUX6H7cd"
      },
      "source": [
        "# Convolutional neural network from scratch\n",
        "\n",
        "Binary classification CNN using just NumPy, linear algebra, and calculus with no PyTorch or TensorFlow.\n",
        "\n",
        "Fully vectorized to ensure efficient inference and training.\n",
        "\n",
        "[GitHub repo](https://github.com/MaximusThomas/ML-Projects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a86001",
      "metadata": {
        "id": "79a86001"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5470f024",
      "metadata": {
        "id": "5470f024"
      },
      "outputs": [],
      "source": [
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ApFvTuVRKGsO",
      "metadata": {
        "id": "ApFvTuVRKGsO"
      },
      "outputs": [],
      "source": [
        "mem_pool = {}\n",
        "def get_buffer(name, shape, dtype=np.float32):\n",
        "\n",
        "    key = (name, shape)\n",
        "    if key not in mem_pool:\n",
        "        mem_pool[key] = np.zeros(shape, dtype=dtype)\n",
        "    else:\n",
        "        mem_pool[key].fill(0)\n",
        "    return mem_pool[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oYwxSO2G7g2b",
      "metadata": {
        "id": "oYwxSO2G7g2b"
      },
      "outputs": [],
      "source": [
        "def vectorized_convolution(A_prev, filters, biases, n_s, n_p):\n",
        "    '''\n",
        "    Single convolutional layer of all filters, assuming ReLU activation\n",
        "\n",
        "    Args:\n",
        "        A_prev ((m, n_H, n_W, n_C)): matrix of activations\n",
        "        filters ((n_filters, n_f, n_f, n_C)): filters\n",
        "        biases (n_filters): bias term for each filter\n",
        "        n_s (int): convolution stride\n",
        "        n_p (int): amount of padding (assumes square symmetrical)\n",
        "\n",
        "    Returns:\n",
        "        A_next ((m, n_H_, n_W_, n_filters)): output matrix of activations\n",
        "        cache (dict): Python dictionary containing keys \"A_prev\", \"A_next\", \"filters\", \"biases\", \"stride\", \"padding\", \"Z\"\n",
        "    '''\n",
        "    # Get filter shape and activation height and width\n",
        "    n_f, n_filters = filters.shape[1], filters.shape[0]\n",
        "    (m, n_H, n_W, n_C) = A_prev.shape\n",
        "\n",
        "    # Calculate output height and width\n",
        "    n_H_ = int(np.floor((n_H + (2 * n_p) - n_f) / n_s)) + 1\n",
        "    n_W_ = int(np.floor((n_W + (2 * n_p) - n_f) / n_s)) + 1\n",
        "\n",
        "    padded_shape = (m, n_H + (2 * n_p), n_W + (2 * n_p), n_C)\n",
        "    A_prev_pad = get_buffer(\"conv_fwd_pad\", padded_shape, A_prev.dtype)\n",
        "    A_prev_pad[:, n_p:n_p+n_H, n_p:n_p+n_W, :] = A_prev\n",
        "\n",
        "    # Create view of sliding windows\n",
        "    shape = (m, n_H_, n_W_, n_f, n_f, n_C)\n",
        "    strides = (A_prev_pad.strides[0], n_s * A_prev_pad.strides[1], n_s * A_prev_pad.strides[2],\n",
        "               A_prev_pad.strides[1], A_prev_pad.strides[2], A_prev_pad.strides[3])\n",
        "    windows = np.lib.stride_tricks.as_strided(A_prev_pad, shape=shape, strides=strides)\n",
        "\n",
        "    # 'mhwfgc' = batch, height, width, filter_h, filter_w, channels\n",
        "    # 'dfgc' = n_filters, filter_h, filter_w, channels\n",
        "    Z_reshaped = np.einsum('mhwfgc,dfgc->mhwd', windows, filters) + biases\n",
        "\n",
        "    # ReLU activation function\n",
        "    A_next = np.maximum(Z_reshaped, 0)\n",
        "\n",
        "    # Cache important values for backprop\n",
        "    cache = {\n",
        "        'A_prev': A_prev, 'filters': filters, 'biases': biases,\n",
        "        'stride': n_s, 'padding': n_p, 'windows': windows, 'Z': Z_reshaped\n",
        "    }\n",
        "\n",
        "    return A_next, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k5V90cxLAwEk",
      "metadata": {
        "id": "k5V90cxLAwEk"
      },
      "outputs": [],
      "source": [
        "def max_pool_vectorized(A_prev, pool_size, n_s):\n",
        "    '''\n",
        "    Max pooling step across all examples and filters\n",
        "\n",
        "    Args:\n",
        "        A_prev ((m, n_H, n_W, n_C)): matrix of activations\n",
        "        pool_size (int): size of pool, assuming square\n",
        "        n_s (int): stride\n",
        "\n",
        "    Returns:\n",
        "        A_next ((m, n_H_, n_W_, n_filters)): pooled output matrix of activations\n",
        "        cache (dict): Python dictionary containing keys \"A_prev\", \"pool_size\", \"stride\"\n",
        "    '''\n",
        "\n",
        "    # Calculate output dimensions\n",
        "    (m, n_H, n_W, n_C) = A_prev.shape\n",
        "    n_H_ = int(np.floor((n_H - pool_size) / n_s)) + 1\n",
        "    n_W_ = int(np.floor((n_W - pool_size) / n_s)) + 1\n",
        "\n",
        "    # Cache values for backprop\n",
        "    cache = {}\n",
        "    cache['A_prev'], cache['pool_size'], cache['stride'] = A_prev, pool_size, n_s\n",
        "\n",
        "    # Create windows\n",
        "    shape = (m, n_H_, n_W_, pool_size, pool_size, n_C)\n",
        "    strides = (A_prev.strides[0],\n",
        "               n_s * A_prev.strides[1],\n",
        "               n_s * A_prev.strides[2],\n",
        "               A_prev.strides[1],\n",
        "               A_prev.strides[2],\n",
        "               A_prev.strides[3])\n",
        "\n",
        "    windows = np.lib.stride_tricks.as_strided(A_prev, shape, strides)\n",
        "\n",
        "    # Take max over each max pooling position\n",
        "    A_next = np.max(windows.reshape(m, n_H_, n_W_, pool_size * pool_size, n_C), axis=3)\n",
        "\n",
        "    return A_next, cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N2f4qPSBCpSJ",
      "metadata": {
        "id": "N2f4qPSBCpSJ"
      },
      "outputs": [],
      "source": [
        "def vec_conv_back_prop(dA_next, cache):\n",
        "    '''\n",
        "    Back prop for a single convolution layer\n",
        "\n",
        "    Args:\n",
        "        dA ((m, n_H_, n_W_, n_filters)): gradient of ReLU activations\n",
        "        cache (): cache of A_prev, filters (weights), stride, and padding\n",
        "    '''\n",
        "\n",
        "    # Retrieve cached values\n",
        "    W, n_s, n_p, Z, windows, A_prev = (\n",
        "        cache['filters'], cache['stride'], cache['padding'],\n",
        "        cache['Z'], cache['windows'], cache['A_prev']\n",
        "    )\n",
        "\n",
        "    n_f = W.shape[1]\n",
        "\n",
        "    # Differentiate ReLU\n",
        "    dZ = dA_next * (Z > 0)\n",
        "\n",
        "    # Optimization with np.einsum\n",
        "    dW = np.einsum('mhwd,mhwfgc->dfgc', dZ, windows)\n",
        "    db = np.sum(dZ, axis=(0, 1, 2), keepdims=True)\n",
        "\n",
        "    # Get d_windows (m, n_H_out, n_W_out, n_f, n_f, n_C)\n",
        "    d_windows = np.einsum('mhwd,dfgc->mhwfgc', dZ, W)\n",
        "\n",
        "    # col2im using pre-allocation and slicing\n",
        "    (m, n_H_A, n_W_A, n_C_A) = A_prev.shape\n",
        "    pad_shape = (m, n_H_A + (2 * n_p), n_W_A + (2 * n_p), n_C_A)\n",
        "    dA_prev_pad = get_buffer(\"conv_back_pad\", pad_shape, A_prev.dtype)\n",
        "\n",
        "    for i in range(n_f):\n",
        "        for j in range(n_f):\n",
        "            dA_prev_pad[:, i:i+d_windows.shape[1]*n_s:n_s, j:j+d_windows.shape[2]*n_s:n_s, :] += \\\n",
        "                d_windows[:, :, :, i, j, :]\n",
        "\n",
        "    # Remove padding\n",
        "    if n_p > 0:\n",
        "        dA_prev = dA_prev_pad[:, n_p:n_H_A+n_p, n_p:n_W_A+n_p, :].copy()\n",
        "    else:\n",
        "        dA_prev = dA_prev_pad\n",
        "\n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9LwrHPFwDHZ4",
      "metadata": {
        "id": "9LwrHPFwDHZ4"
      },
      "outputs": [],
      "source": [
        "def vec_max_pool_backprop(dA_next, A_prev, pool_size, n_s):\n",
        "  '''\n",
        "  Backprop for a max pooling layer\n",
        "\n",
        "  Args:\n",
        "      dA_next ((m, n_H_, n_W_, n_filters)): tensor of activations from max pooling layer\n",
        "      pool_size (int): size of pool, assuming square\n",
        "      n_s (int): stride\n",
        "\n",
        "  Returns:\n",
        "      dA_prev ((m, n_H, n_W, n_C)): tensor of activations from previous conv layer\n",
        "  '''\n",
        "\n",
        "  # Retrieve shapes\n",
        "  # dA_next is (m, n_H_out, n_W_out, n_C) from full_backprop after reshape\n",
        "  (m, n_H_out, n_W_out, n_C) = dA_next.shape\n",
        "  (m_A_prev, n_H_A_prev, n_W_A_prev, n_C_A_prev) = A_prev.shape\n",
        "\n",
        "  # Calculate output dimensions for pooling based on A_prev\n",
        "  n_H_pool = int(np.floor((n_H_A_prev - pool_size) / n_s)) + 1\n",
        "  n_W_pool = int(np.floor((n_W_A_prev - pool_size) / n_s)) + 1\n",
        "\n",
        "  # Create windows view on A_prev for mask generation\n",
        "  shape_windows = (m_A_prev, n_H_pool, n_W_pool, pool_size, pool_size, n_C_A_prev)\n",
        "  strides_windows = (A_prev.strides[0],\n",
        "                     n_s * A_prev.strides[1],\n",
        "                     n_s * A_prev.strides[2],\n",
        "                     A_prev.strides[1],\n",
        "                     A_prev.strides[2],\n",
        "                     A_prev.strides[3])\n",
        "  windows = np.lib.stride_tricks.as_strided(A_prev, shape=shape_windows, strides=strides_windows)\n",
        "\n",
        "  # Create mask for max values (indices where max was taken in forward pass)\n",
        "  A_prev_flat = windows.reshape(m_A_prev, n_H_pool, n_W_pool, pool_size * pool_size, n_C_A_prev)\n",
        "  arg_max = np.argmax(A_prev_flat, axis=3)\n",
        "  mask = np.zeros_like(A_prev_flat)\n",
        "\n",
        "  # Indexing to set only the first max occurrence to 1\n",
        "  m_idx, h_idx, w_idx, c_idx = np.indices((m_A_prev, n_H_pool, n_W_pool, n_C_A_prev))\n",
        "  mask[m_idx, h_idx, w_idx, arg_max, c_idx] = 1\n",
        "\n",
        "  # Reshape dA_next for broadcasting with mask\n",
        "  dA_next_reshaped_for_mask = dA_next[:, :, :, np.newaxis, :] # (m, n_H_out, n_W_out, 1, n_C)\n",
        "\n",
        "  # Apply mask to distribute gradients only to the maximum elements\n",
        "  dA_col_flat = mask * dA_next_reshaped_for_mask # (m, n_H_pool, n_W_pool, pool_size*pool_size, n_C)\n",
        "  dA_col = dA_col_flat.reshape(m_A_prev, n_H_pool, n_W_pool, pool_size, pool_size, n_C_A_prev) # (m, n_H_pool, n_W_pool, pool_size, pool_size, n_C)\n",
        "\n",
        "  # col2im: Initialize dA_prev with zeros, then accumulate gradients\n",
        "  dA_prev = np.zeros_like(A_prev)\n",
        "  for i in range(pool_size):\n",
        "      for j in range(pool_size):\n",
        "          dA_prev[:, i:i+n_H_pool*n_s:n_s, j:j+n_W_pool*n_s:n_s, :] += dA_col[:, :, :, i, j, :]\n",
        "\n",
        "  return dA_prev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d6641d9",
      "metadata": {
        "id": "7d6641d9"
      },
      "outputs": [],
      "source": [
        "def softmax(Z):\n",
        "    exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
        "    return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c6b8cf4",
      "metadata": {
        "id": "7c6b8cf4"
      },
      "outputs": [],
      "source": [
        "def categorical_cross_entropy_cost(A, Y, epsilon=1e-8):\n",
        "\n",
        "    m = Y.shape[1]\n",
        "    A = np.clip(A, epsilon, 1 - epsilon)\n",
        "    cost = -1/m * np.sum(Y * np.log(A))\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8723fdb9",
      "metadata": {
        "id": "8723fdb9"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(X, dense_dims, conv_params, conv_dims):\n",
        "\n",
        "    m = X.shape[0]\n",
        "    A_prev = X\n",
        "    l = 1\n",
        "    for dim in conv_dims:\n",
        "\n",
        "      if dim == 'conv' or 'conv' in dim:\n",
        "\n",
        "        # Single convolutional layer with ReLU activation\n",
        "        A_next, _ = vectorized_convolution(A_prev,\n",
        "                                    filters=conv_params['W' + str(l)],\n",
        "                                    biases=conv_params['b' + str(l)],\n",
        "                                    n_s=conv_dims[dim]['n_s'],\n",
        "                                    n_p=conv_dims[dim]['n_p'])\n",
        "        l += 1\n",
        "\n",
        "      elif dim == 'max_pool' or 'pool' in dim:\n",
        "        A_next, _ = max_pool_vectorized(A_prev, conv_dims[dim]['pool_size'], conv_dims[dim]['n_s'])\n",
        "\n",
        "      A_prev = A_next\n",
        "\n",
        "    dense_dims_local = dense_dims.copy()\n",
        "    dense_dims_local.insert(0, A_prev.reshape(m, -1).shape[1])\n",
        "\n",
        "    parameters = {}\n",
        "    for l in range(1, len(dense_dims_local)):\n",
        "        parameters['W' + str(l)] = np.random.randn(dense_dims_local[l], dense_dims_local[l - 1]).astype(np.float32) * np.sqrt(2/dense_dims_local[l-1]).astype(np.float32)\n",
        "        parameters['b' + str(l)] = np.zeros((dense_dims_local[l], 1)).astype(np.float32)\n",
        "\n",
        "    return parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5819464c",
      "metadata": {
        "id": "5819464c"
      },
      "outputs": [],
      "source": [
        "def dense_forward_prop(conv_prev, y, parameters):\n",
        "    '''\n",
        "    Forward prop for dense layers, flattening convolutional output, assuming ReLU activation and sigmoid output activation\n",
        "\n",
        "    Args:\n",
        "        conv_prev ((m, n_H, n_W, n_filters)): tensor of activations from final max pooling layer\n",
        "        parameters (dict): dictionary containing all dense network parameters\n",
        "            W ((n_l, n_l_prev)): matrix of weights\n",
        "            b ((n_l, 1)): column vector of bias terms\n",
        "        y ((1, m)): labels\n",
        "\n",
        "    Returns:\n",
        "        yhat ((1, m)): predictions from forward prop\n",
        "    '''\n",
        "\n",
        "    # Flatten A_prev into a 2D matrix (Features x Batch)\n",
        "    m = conv_prev.shape[0]\n",
        "    A_prev = conv_prev.reshape(m, -1).T\n",
        "\n",
        "    # Forward prop through dense network\n",
        "    caches = []\n",
        "    L = len(parameters) // 2\n",
        "    for l in range(1, L):\n",
        "        cache = {}\n",
        "\n",
        "        # Calculate linear product\n",
        "        Z = np.dot(parameters['W' + str(l)], A_prev) + parameters['b' + str(l)]\n",
        "        cache['Z' + str(l)] = Z\n",
        "        cache['W' + str(l)] = parameters['W' + str(l)]\n",
        "\n",
        "        # Apply activation function\n",
        "        A_next = np.maximum(Z, 0)\n",
        "        cache['A' + str(l-1)] = A_prev\n",
        "        A_prev = A_next\n",
        "\n",
        "        # Append cache\n",
        "        caches.append(cache)\n",
        "\n",
        "    # Output layer\n",
        "    cache = {}\n",
        "    cache['A' + str(L-1)] = A_prev\n",
        "    Z = np.dot(parameters['W' + str(L)], A_prev) + parameters['b' + str(L)]\n",
        "    cache['Z' + str(L)] = Z\n",
        "    cache['W' + str(L)] = parameters['W' + str(L)]\n",
        "    caches.append(cache)\n",
        "    yhat = softmax(Z)\n",
        "\n",
        "    # Calculate loss\n",
        "    cost = categorical_cross_entropy_cost(yhat, y)\n",
        "\n",
        "    return cost, yhat, caches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc2415a6",
      "metadata": {
        "id": "cc2415a6"
      },
      "outputs": [],
      "source": [
        "def dense_backprop(yhat, y, caches):\n",
        "    '''\n",
        "    Backprop for dense layers, assuming ReLU activation and sigmoid output activation\n",
        "\n",
        "    Args:\n",
        "        yhat ((1, m)): vector of activations of output layer for each example\n",
        "        y ((1, m)): vector of labels for each training example\n",
        "        caches (list): list of each layer's cache dictionary\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    '''\n",
        "\n",
        "    m = yhat.shape[1]\n",
        "    grads = {}\n",
        "    L = len(caches)\n",
        "\n",
        "    # Backprop for output layer\n",
        "    cache = caches[-1]\n",
        "    grads['dZ' + str(L)] = yhat - y\n",
        "    grads['dW' + str(L)] = 1/m * np.dot(grads['dZ' + str(L)], cache['A' + str(L-1)].T)\n",
        "    grads['db' + str(L)] = 1/m * np.sum(grads['dZ' + str(L)], axis=1, keepdims=True)\n",
        "    grads['dA' + str(L-1)] = np.dot(cache['W' + str(L)].T, grads['dZ' + str(L)])\n",
        "\n",
        "    # Backprop for hidden layers\n",
        "    for l in range(L-1, 0, -1):\n",
        "\n",
        "      cache = caches[l-1]\n",
        "      dZ = np.array(grads['dA' + str(l)], copy=True)\n",
        "      dZ[cache['Z' + str(l)] <= 0] = 0\n",
        "      grads['dZ' + str(l)] = dZ\n",
        "\n",
        "      grads['dW' + str(l)] = 1/m * np.dot(grads['dZ' + str(l)], cache['A' + str(l-1)].T)\n",
        "      grads['db' + str(l)] = 1/m * np.sum(grads['dZ' + str(l)], axis=1, keepdims=True)\n",
        "      grads['dA' + str(l-1)] = np.dot(cache['W' + str(l)].T, grads['dZ' + str(l)])\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c84f820f",
      "metadata": {
        "id": "c84f820f"
      },
      "outputs": [],
      "source": [
        "def initialize_convolution(X, conv_dims):\n",
        "    '''\n",
        "    Initializes parameters for convolutional part of network\n",
        "\n",
        "    Args:\n",
        "        X (((m, n_H, n_W, n_C)): input matrix\n",
        "        conv_dims (dict): dict containing either 'conv', or 'max_pool' corresponding to a sub-dict\n",
        "            conv (dict): dict with keys 'n_f', 'n_s', 'n_p', 'n_filters'\n",
        "                n_f (int): kernel/filter size\n",
        "                n_s (int): stride size\n",
        "                n_p (int): padding size\n",
        "                n_filters (int): number of filters\n",
        "            max_pool (dict): dict with keys 'pool_size', 'n_s'\n",
        "                pool_size (int): max pooling size\n",
        "                n_s (int): max pooling stride\n",
        "\n",
        "    Returns:\n",
        "        conv_params (dict): dict containg keys 'Wl' (filters of layer l), and 'bl' (biases of layer l)\n",
        "    '''\n",
        "\n",
        "    conv_params = {}\n",
        "    n_C = X.shape[-1]\n",
        "    l = 1\n",
        "\n",
        "    for conv_dim in conv_dims:\n",
        "        if conv_dim == 'conv' or 'conv' in conv_dim:\n",
        "            n_filters, n_f = conv_dims[conv_dim]['n_filters'], conv_dims[conv_dim]['n_f']\n",
        "            conv_params['W' + str(l)] = np.random.randn(n_filters, n_f, n_f, n_C) * np.sqrt(2.0 / (n_f * n_f * n_C))\n",
        "            conv_params['b' + str(l)] = np.zeros((1, 1, 1, n_filters)).astype(np.float32)\n",
        "            n_C = n_filters\n",
        "            l += 1\n",
        "    return conv_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xc3qJMeS3Whq",
      "metadata": {
        "id": "Xc3qJMeS3Whq"
      },
      "outputs": [],
      "source": [
        "def full_forwardprop(X, y, conv_dims, dense_dims, params):\n",
        "    '''\n",
        "    Forward prop for both convolutional and dense sections of the network, assuming ReLU activation\n",
        "\n",
        "    Args:\n",
        "        X ((m, n_H, n_W, n_C)): input matrix\n",
        "        conv_dims (dict): dict containing either 'conv', or 'max_pool' corresponding to a sub-dict\n",
        "            conv (dict): dict with keys 'n_f', 'n_s', 'n_p', 'n_filters'\n",
        "                n_f (int): kernel/filter size\n",
        "                n_s (int): stride size\n",
        "                n_p (int): padding size\n",
        "                n_filters (int): number of filters\n",
        "            max_pool (dict): dict with keys 'pool_size', 'n_s'\n",
        "                pool_size (int): max pooling size\n",
        "                n_s (int): max pooling stride\n",
        "\n",
        "    Returns:\n",
        "        yhat ((1, m)): predictions from forward prop\n",
        "    '''\n",
        "    (conv_params, dense_params) = params\n",
        "    A_prev = X\n",
        "    conv_caches = []\n",
        "    l = 1\n",
        "\n",
        "    # Enumerate through both convolutional and max pooling layers\n",
        "    for dim in conv_dims:\n",
        "\n",
        "      if dim == 'conv' or 'conv' in dim:\n",
        "\n",
        "        # Single convolutional layer with ReLU activation\n",
        "        A_next, cache = vectorized_convolution(A_prev,\n",
        "                                    filters=conv_params['W' + str(l)],\n",
        "                                    biases=conv_params['b' + str(l)],\n",
        "                                    n_s=conv_dims[dim]['n_s'],\n",
        "                                    n_p=conv_dims[dim]['n_p'])\n",
        "        l += 1\n",
        "\n",
        "      elif dim == 'max_pool' or 'pool' in dim:\n",
        "        A_next, cache = max_pool_vectorized(A_prev, conv_dims[dim]['pool_size'], conv_dims[dim]['n_s'])\n",
        "\n",
        "      conv_caches.append(cache)\n",
        "      A_prev = A_next\n",
        "\n",
        "    # Forward prop through dense layers\n",
        "    cost, yhat, dense_caches = dense_forward_prop(A_prev, y, dense_params)\n",
        "\n",
        "    return cost, yhat, dense_caches, conv_caches, A_next"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ry4Vr1FtxTAe",
      "metadata": {
        "id": "ry4Vr1FtxTAe"
      },
      "outputs": [],
      "source": [
        "def full_backprop(yhat, y, dense_caches, conv_caches, conv_dims, A_next):\n",
        "    '''\n",
        "    Backprop for both the dense and convolutional sections of the nework\n",
        "\n",
        "    Args:\n",
        "        yhat ((1, m)): vector of activations of output layer for each example\n",
        "        y ((1, m)): vector of labels for each training example\n",
        "        caches (list): list of each layer's cache dictionary\n",
        "          cache (dict): Python dictionary containing\n",
        "            A_prev ((m, n_H, n_W, n_C)): previous activation matrices\n",
        "            filters ((n_filters, n_f, n_f, n_C)): filters\n",
        "            biases (n_filters, 1): bias term for each filter\n",
        "            stride (int): stride size\n",
        "            padding (int): padding size\n",
        "        conv_dims (dict): dict containing either 'conv', or 'max_pool' corresponding to a sub-dict\n",
        "            conv (dict): dict with keys 'n_f', 'n_s', 'n_p', 'n_filters'\n",
        "                n_f (int): kernel/filter size\n",
        "                n_s (int): stride size\n",
        "                n_p (int): padding size\n",
        "                n_filters (int): number of filters\n",
        "            max_pool (dict): dict with keys 'pool_size', 'n_s'\n",
        "                pool_size (int): max pooling size\n",
        "                n_s (int): max pooling stride\n",
        "\n",
        "    Returns:\n",
        "        dense_grads (dict): dict containg keys 'W + str(l)' and 'b + str(l)\n",
        "        conv_grads (dict): dict containing keys 'W + str(l)' and 'b + str(l)\n",
        "    '''\n",
        "\n",
        "    # Full dense backprop\n",
        "    dense_grads = dense_backprop(yhat, y, dense_caches)\n",
        "\n",
        "    # Get gradient for output of dense section and reshape back to 4D\n",
        "    shape = A_next.shape\n",
        "    dA_next = dense_grads['dA0'].T.reshape(shape)\n",
        "\n",
        "    conv_grads = {}\n",
        "\n",
        "    # Create a mapping from cache index to conv layer number used in params\n",
        "    conv_layer_numbers = []\n",
        "    current_conv_l = 1\n",
        "    for dim_type in conv_dims:\n",
        "\n",
        "      if 'conv' in dim_type:\n",
        "        conv_layer_numbers.append(current_conv_l)\n",
        "        current_conv_l += 1\n",
        "      else: # For pooling layers, no conv param number\n",
        "        conv_layer_numbers.append(None)\n",
        "\n",
        "    # Iterate through conv_caches in reverse order for backpropagation\n",
        "    dA_prev = None\n",
        "    for cache_idx in range(len(conv_caches) - 1, -1, -1):\n",
        "      conv_cache = conv_caches[cache_idx]\n",
        "      dim_type = list(conv_dims.keys())[cache_idx]\n",
        "\n",
        "      # Change exact match to 'in' check\n",
        "      if 'conv' in dim_type:\n",
        "          conv_layer_num = conv_layer_numbers[cache_idx]\n",
        "          dA_prev, dW, db = vec_conv_back_prop(dA_next, conv_cache)\n",
        "          conv_grads['dW' + str(conv_layer_num)] = dW\n",
        "          conv_grads['db' + str(conv_layer_num)] = db\n",
        "\n",
        "      elif 'pool' in dim_type:\n",
        "          A_prev, pool_size, n_s = conv_cache['A_prev'], conv_cache['pool_size'], conv_cache['stride']\n",
        "          dA_prev = vec_max_pool_backprop(dA_next, A_prev, pool_size, n_s)\n",
        "\n",
        "      # Update dA_next\n",
        "      dA_next = dA_prev\n",
        "\n",
        "    # Return gradient dictionaries\n",
        "    return conv_grads, dense_grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gsrck3Sws9iv",
      "metadata": {
        "id": "gsrck3Sws9iv"
      },
      "outputs": [],
      "source": [
        "def Adam(params, grads, S, V, iter, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    '''\n",
        "    Adam optimization algorithm, combining both RMSprop and gradient descent with momentum, as well as bias correction\n",
        "\n",
        "    Args:\n",
        "      parameters (tuple): tuple of conv_params and dense_params dictionaries\n",
        "        conv_params (dict): dict containg keys 'Wl' (filters of layer l), and 'bl' (biases of layer l)\n",
        "        dense_params (dict): dict with keys 'Wl' (weights of layer l) and 'bl' (biases of layer l)\n",
        "      grads (tuple): tuple of conv_grads and dense_grads dicts\n",
        "        conv_grads (dict): dict with keys 'dWl' and 'dbl' (i.e., the partial derivative of each parameter)\n",
        "        dense_grads (dict): dict with keys 'dWl' and 'dbl' (i.e., the partial derivative of each parameter)\n",
        "      S (tuple): contains exponential moving average of squared gradients dictionaries of conv_S and dense_S\n",
        "      V (tuple): contains exponential moving average of gradients dictionaries of conv_V and dense_V\n",
        "\n",
        "    Returns:\n",
        "      parameters (tuple): updated tuple of conv_params and dense_params dictionaries\n",
        "        conv_params (dict): updated dict containg keys 'Wl' (filters of layer l), and 'bl' (biases of layer l)\n",
        "        dense_params (dict): updated dict with keys 'Wl' (weights of layer l) and 'bl' (biases of layer l)\n",
        "      S (tuple): contains exponential moving average of squared gradients dictionaries of conv_S and dense_S\n",
        "      V (tuple): contains exponential moving average of gradients dictionaries of conv_V and dense_V\n",
        "    '''\n",
        "\n",
        "    # Unpack tuples\n",
        "    (conv_params, dense_params) = params\n",
        "    (conv_grads, dense_grads) = grads\n",
        "    (conv_S, dense_S) = S\n",
        "    (conv_V, dense_V) = V\n",
        "\n",
        "    # Iterate over all dense_params\n",
        "    L = len(dense_params)//2\n",
        "    for l in range(1, L+1):\n",
        "\n",
        "        # Get gradients\n",
        "        dWl, dbl = dense_grads['dW'+str(l)], dense_grads['db'+str(l)]\n",
        "\n",
        "        # Update velocity terms\n",
        "        dense_V['dW'+str(l)] = (beta1 * dense_V['dW'+str(l)]) + ((1-beta1) * dWl)\n",
        "        dense_V['db'+str(l)] = (beta1 * dense_V['db'+str(l)]) + ((1-beta1) * dbl)\n",
        "\n",
        "        # Update squared-gradient terms\n",
        "        dense_S['dW' + str(l)] = (beta2 * dense_S['dW' + str(l)]) + ((1 - beta2) * np.square(dWl))\n",
        "        dense_S['db' + str(l)] = (beta2 * dense_S['db' + str(l)]) + ((1 - beta2) * np.square(dbl))\n",
        "\n",
        "        # Bias correction\n",
        "        vdW_corrected = dense_V['dW'+str(l)] / (1 - (beta1 ** iter))\n",
        "        vdb_corrected = dense_V['db'+str(l)] / (1 - (beta1 ** iter))\n",
        "\n",
        "        sdW_corrected = dense_S['dW' + str(l)] / (1 - (beta2 ** iter))\n",
        "        sdb_corrected = dense_S['db' + str(l)] / (1 - (beta2 ** iter))\n",
        "\n",
        "        # Update Parameters\n",
        "        dense_params['W'+str(l)] -= (alpha * vdW_corrected) / (np.sqrt(sdW_corrected)+epsilon)\n",
        "        dense_params['b'+str(l)] -= (alpha * vdb_corrected) / (np.sqrt(sdb_corrected)+epsilon)\n",
        "\n",
        "  # Iterate over all conv_params\n",
        "    L = len(conv_params)//2\n",
        "    for l in range(1, L+1):\n",
        "\n",
        "        # Get gradients\n",
        "        dWl, dbl = conv_grads['dW'+str(l)], conv_grads['db'+str(l)]\n",
        "\n",
        "        # Update velocity terms\n",
        "        conv_V['dW'+str(l)] = (beta1 * conv_V['dW'+str(l)]) + ((1-beta1) * dWl)\n",
        "        conv_V['db'+str(l)] = (beta1 * conv_V['db'+str(l)]) + ((1-beta1) * dbl)\n",
        "\n",
        "        # Update squared-gradient terms\n",
        "        conv_S['dW' + str(l)] = (beta2 * conv_S['dW' + str(l)]) + ((1 - beta2) * np.square(dWl))\n",
        "        conv_S['db' + str(l)] = (beta2 * conv_S['db' + str(l)]) + ((1 - beta2) * np.square(dbl))\n",
        "\n",
        "        # Bias correction\n",
        "        vdW_corrected = conv_V['dW'+str(l)] / (1 - (beta1 ** iter))\n",
        "        vdb_corrected = conv_V['db'+str(l)] / (1 - (beta1 ** iter))\n",
        "\n",
        "        sdW_corrected = conv_S['dW' + str(l)] / (1 - (beta2 ** iter))\n",
        "        sdb_corrected = conv_S['db' + str(l)] / (1 - (beta2 ** iter))\n",
        "\n",
        "        # Update Parameters\n",
        "        conv_params['W'+str(l)] -= (alpha * vdW_corrected) / (np.sqrt(sdW_corrected)+epsilon)\n",
        "        conv_params['b'+str(l)] -= (alpha * vdb_corrected) / (np.sqrt(sdb_corrected)+epsilon)\n",
        "\n",
        "    # Repack tuples\n",
        "    params = (conv_params, dense_params)\n",
        "    S = (conv_S, dense_S)\n",
        "    V = (conv_V, dense_V)\n",
        "\n",
        "    return params, S, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0xL_V05r0xNJ",
      "metadata": {
        "id": "0xL_V05r0xNJ"
      },
      "outputs": [],
      "source": [
        "def initialize_adam(params):\n",
        "\n",
        "  # Initialize adam\n",
        "  (conv_params, dense_params) = params\n",
        "  dense_S, conv_S, dense_V, conv_V = {}, {}, {}, {}\n",
        "  S, V = (conv_S, dense_S), (conv_V, dense_V)\n",
        "\n",
        "  # Loop over dense params\n",
        "  L = len(dense_params)//2\n",
        "  for l in range(1, L+1):\n",
        "\n",
        "    # RMS prop terms\n",
        "    dense_S['dW'+str(l)] = np.zeros_like(dense_params['W'+str(l)]).astype(np.float32)\n",
        "    dense_S['db'+str(l)] = np.zeros_like(dense_params['b'+str(l)]).astype(np.float32)\n",
        "\n",
        "    # Momentum terms\n",
        "    dense_V['dW'+str(l)] = np.zeros_like(dense_params['W'+str(l)]).astype(np.float32)\n",
        "    dense_V['db'+str(l)] = np.zeros_like(dense_params['b'+str(l)]).astype(np.float32)\n",
        "\n",
        "  L = len(conv_params)//2\n",
        "  for l in range(1, L+1):\n",
        "    # RMS prop terms\n",
        "    conv_S['dW'+str(l)] = np.zeros_like(conv_params['W'+str(l)]).astype(np.float32)\n",
        "    conv_S['db'+str(l)] = np.zeros_like(conv_params['b'+str(l)]).astype(np.float32)\n",
        "\n",
        "    # Momentum terms\n",
        "    conv_V['dW'+str(l)] = np.zeros_like(conv_params['W'+str(l)]).astype(np.float32)\n",
        "    conv_V['db'+str(l)] = np.zeros_like(conv_params['b'+str(l)]).astype(np.float32)\n",
        "\n",
        "  S, V = (conv_S, dense_S), (conv_V, dense_V)\n",
        "\n",
        "  return S, V\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FlZ56QoVr4Ds",
      "metadata": {
        "id": "FlZ56QoVr4Ds"
      },
      "outputs": [],
      "source": [
        "def get_learning_rate(epoch, initial_lr=0.001):\n",
        "    if epoch < 10:\n",
        "        return initial_lr\n",
        "    elif epoch < 20:\n",
        "        return initial_lr * 0.1\n",
        "    else:\n",
        "        return initial_lr * 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a_aDEG4mypSg",
      "metadata": {
        "id": "a_aDEG4mypSg"
      },
      "outputs": [],
      "source": [
        "def train(X, y, X_test, y_test, dims, epochs, batch_size=64):\n",
        "    '''\n",
        "    Forward and backprop to train CNN model\n",
        "\n",
        "    Args:\n",
        "        X ((m, n_H, n_W, n_C)): inputs\n",
        "    '''\n",
        "\n",
        "    (conv_dims, dense_dims) = dims\n",
        "    conv_params = initialize_convolution(X, conv_dims)\n",
        "    dense_params = initialize_parameters(X, dense_dims, conv_params, conv_dims)\n",
        "    params = (conv_params, dense_params)\n",
        "    S, V = initialize_adam(params)\n",
        "\n",
        "    # Empty list to record cost\n",
        "    cost_history_train, cost_history_test = [], []\n",
        "    m = X.shape[0]\n",
        "\n",
        "    iter = 0\n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(1, epochs+1)):\n",
        "\n",
        "      permutation = np.random.permutation(m)\n",
        "      X_shuffled, y_shuffled = X[permutation], y[:, permutation]\n",
        "      epoch_cost_train = 0\n",
        "      num_batches = int(np.ceil(m / batch_size))\n",
        "\n",
        "      for i in range(num_batches):\n",
        "        begin = i * batch_size\n",
        "        end = min(begin + batch_size, m)\n",
        "\n",
        "        X_batch, y_batch = X_shuffled[begin:end], y_shuffled[:, begin:end]\n",
        "\n",
        "        # Full forward prop\n",
        "        cost, yhat, dense_caches, conv_caches, A_next = full_forwardprop(X_batch, y_batch, conv_dims, dense_dims, params)\n",
        "        epoch_cost_train += cost\n",
        "\n",
        "        # Full backprop\n",
        "        conv_grads, dense_grads = full_backprop(yhat, y_batch, dense_caches, conv_caches, conv_dims, A_next)\n",
        "\n",
        "        iter += 1\n",
        "        # Adam optimization algorithm\n",
        "        lr = get_learning_rate(epoch)\n",
        "        params, S, V = Adam(params, (conv_grads, dense_grads), S, V, iter, alpha=lr)\n",
        "        del conv_caches, dense_caches, A_next\n",
        "\n",
        "      cost_history_train.append(epoch_cost_train / num_batches)\n",
        "\n",
        "      cost_test, _ = predict(X_test, y_test, params, dims)\n",
        "      cost_history_test.append(cost_test)\n",
        "\n",
        "      cost_history = (cost_history_train, cost_history_test)\n",
        "\n",
        "    return params, cost_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gl4TYkI8-_7T",
      "metadata": {
        "id": "gl4TYkI8-_7T"
      },
      "outputs": [],
      "source": [
        "def conv_forward_no_cache(A_prev, filters, biases, n_s, n_p):\n",
        "\n",
        "    # Get filter shape and activation dimensions\n",
        "    n_f, n_filters = filters.shape[1], filters.shape[0]\n",
        "    (m, n_H, n_W, n_C) = A_prev.shape\n",
        "\n",
        "    # Calculate output dimensions\n",
        "    n_H_ = int(np.floor((n_H + (2 * n_p) - n_f) / n_s)) + 1\n",
        "    n_W_ = int(np.floor((n_W + (2 * n_p) - n_f) / n_s)) + 1\n",
        "\n",
        "    # Apply padding\n",
        "    padded_shape = (m, n_H + (2 * n_p), n_W + (2 * n_p), n_C)\n",
        "    A_prev_pad = get_buffer(\"pred_conv_pad\", padded_shape, A_prev.dtype)\n",
        "    A_prev_pad[:, n_p:n_p+n_H, n_p:n_p+n_W, :] = A_prev\n",
        "\n",
        "    # Create sliding windows view\n",
        "    shape = (m, n_H_, n_W_, n_f, n_f, n_C)\n",
        "    strides = (A_prev_pad.strides[0], n_s * A_prev_pad.strides[1], n_s * A_prev_pad.strides[2],\n",
        "               A_prev_pad.strides[1], A_prev_pad.strides[2], A_prev_pad.strides[3])\n",
        "    windows = np.lib.stride_tricks.as_strided(A_prev_pad, shape=shape, strides=strides)\n",
        "\n",
        "    # Convolution with einsum\n",
        "    Z = np.einsum('mhwfgc,dfgc->mhwd', windows, filters) + biases\n",
        "\n",
        "    # ReLU activation\n",
        "    A_next = np.maximum(Z, 0)\n",
        "\n",
        "    return A_next\n",
        "\n",
        "def max_pool_forward_no_cache(A_prev, pool_size, n_s):\n",
        "\n",
        "    # Calculate output dimensions\n",
        "    (m, n_H, n_W, n_C) = A_prev.shape\n",
        "    n_H_ = int(np.floor((n_H - pool_size) / n_s)) + 1\n",
        "    n_W_ = int(np.floor((n_W - pool_size) / n_s)) + 1\n",
        "\n",
        "    # Create sliding windows view\n",
        "    shape = (m, n_H_, n_W_, pool_size, pool_size, n_C)\n",
        "    strides = (A_prev.strides[0],\n",
        "               n_s * A_prev.strides[1],\n",
        "               n_s * A_prev.strides[2],\n",
        "               A_prev.strides[1],\n",
        "               A_prev.strides[2],\n",
        "               A_prev.strides[3])\n",
        "    windows = np.lib.stride_tricks.as_strided(A_prev, shape, strides)\n",
        "\n",
        "    # Max pooling\n",
        "    A_next = np.max(windows.reshape(m, n_H_, n_W_, pool_size * pool_size, n_C), axis=3)\n",
        "\n",
        "    return A_next\n",
        "\n",
        "def dense_forward_no_cache(conv_prev, y, parameters):\n",
        "\n",
        "    # Flatten conv output\n",
        "    m = conv_prev.shape[0]\n",
        "    A_prev = conv_prev.reshape(m, -1).T\n",
        "\n",
        "    # Forward through hidden layers\n",
        "    L = len(parameters) // 2\n",
        "    for l in range(1, L):\n",
        "        Z = np.dot(parameters['W' + str(l)], A_prev) + parameters['b' + str(l)]\n",
        "        A_prev = np.maximum(Z, 0)  # ReLU\n",
        "\n",
        "    # Output layer\n",
        "    Z = np.dot(parameters['W' + str(L)], A_prev) + parameters['b' + str(L)]\n",
        "    yhat = softmax(Z)\n",
        "\n",
        "    # Calculate cost\n",
        "    cost = categorical_cross_entropy_cost(yhat, y)\n",
        "\n",
        "    return cost, yhat\n",
        "\n",
        "def predict(X, y, params, dims):\n",
        "    '''\n",
        "    Efficient forward prop for prediction only (no caching for backprop)\n",
        "\n",
        "    Args:\n",
        "        X ((m, n_H, n_W, n_C)): input images\n",
        "        y ((1, m)): labels\n",
        "        params (tuple): (conv_params, dense_params)\n",
        "        dims (tuple): (conv_dims, dense_dims)\n",
        "\n",
        "    Returns:\n",
        "        cost (float): binary cross-entropy cost\n",
        "        yhat ((1, m)): predicted probabilities\n",
        "    '''\n",
        "    (conv_params, dense_params) = params\n",
        "    (conv_dims, dense_dims) = dims\n",
        "\n",
        "    A_prev = X\n",
        "    l = 1\n",
        "\n",
        "    # Forward through conv and pooling layers\n",
        "    for dim in conv_dims:\n",
        "        if dim == 'conv' or 'conv' in dim:\n",
        "            A_next = conv_forward_no_cache(A_prev,\n",
        "                                           filters=conv_params['W' + str(l)],\n",
        "                                           biases=conv_params['b' + str(l)],\n",
        "                                           n_s=conv_dims[dim]['n_s'],\n",
        "                                           n_p=conv_dims[dim]['n_p'])\n",
        "            l += 1\n",
        "        elif dim == 'max_pool' or 'pool' in dim:\n",
        "            A_next = max_pool_forward_no_cache(A_prev,\n",
        "                                               conv_dims[dim]['pool_size'],\n",
        "                                               conv_dims[dim]['n_s'])\n",
        "        A_prev = A_next\n",
        "\n",
        "    # Forward through dense layers\n",
        "    cost, yhat = dense_forward_no_cache(A_prev, y, dense_params)\n",
        "\n",
        "    return cost, np.argmax(yhat, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FvmfFQ0fTe1b",
      "metadata": {
        "id": "FvmfFQ0fTe1b"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(y, num_classes=10):\n",
        "\n",
        "    m = y.shape[0]\n",
        "    one_hot = np.zeros((num_classes, m))\n",
        "    one_hot[y, np.arange(m)] = 1\n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8YE9cpSdqSZF",
      "metadata": {
        "id": "8YE9cpSdqSZF"
      },
      "outputs": [],
      "source": [
        "def batched_predict_with_cleanup(X, y, params, dims, batch_size=512):\n",
        "    preds = np.zeros(X.shape[0], dtype=np.int64)\n",
        "\n",
        "    for i in range(0, X.shape[0], batch_size):\n",
        "        end = min(i + batch_size, X.shape[0])\n",
        "        _, yhat = predict(X[i:end], y[:, i:end], params, dims)\n",
        "        preds[i:end] = yhat\n",
        "\n",
        "        # Clear memory pool every 10 batches\n",
        "        if i % (batch_size * 10) == 0:\n",
        "            mem_pool.clear()\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XADEVct3wUgy",
      "metadata": {
        "id": "XADEVct3wUgy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load data\n",
        "(train_X, train_y), (test_X, test_y) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Reshape X to (m, n_H, n_W, n_C)\n",
        "train_X = train_X.reshape(train_X.shape[0], 28, 28, 1).astype(np.float32)\n",
        "test_X = test_X.reshape(test_X.shape[0], 28, 28, 1).astype(np.float32)\n",
        "\n",
        "# Select subset of data\n",
        "# train_X = train_X[:1000, :, :, :]\n",
        "# train_y = train_y[:1000]\n",
        "# test_X = test_X[:1000, :, :, :]\n",
        "# test_y = test_y[:1000]\n",
        "\n",
        "# Reshape y to (1, m)\n",
        "train_y = one_hot_encode(train_y)\n",
        "test_y = one_hot_encode(test_y)\n",
        "\n",
        "# Normalization\n",
        "train_X = train_X / 255.0\n",
        "test_X = test_X / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "njacxGZ731pl",
      "metadata": {
        "id": "njacxGZ731pl"
      },
      "outputs": [],
      "source": [
        "# Specify architecture\n",
        "conv_1 = {'n_f': 5, 'n_s': 1, 'n_p': 2, 'n_filters': 32}\n",
        "pool_1 = {'pool_size': 2, 'n_s': 2}\n",
        "conv_2 = {'n_f': 5, 'n_s': 1, 'n_p': 2, 'n_filters': 64}\n",
        "pool_2 = {'pool_size': 2, 'n_s': 2}\n",
        "\n",
        "conv_dims = {\n",
        "    'conv': conv_1, 'max_pool': pool_1,\n",
        "    'conv2': conv_2, 'max_pool2': pool_2\n",
        "}\n",
        "dense_dims = [1024, 10].copy()\n",
        "\n",
        "dims = (conv_dims, dense_dims)\n",
        "\n",
        "# Train network\n",
        "params, cost_history = train(train_X, train_y,\n",
        "                             test_X, test_y,\n",
        "                             dims, epochs=25, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8Mkq5fYEpwyV",
      "metadata": {
        "id": "8Mkq5fYEpwyV"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w5Ry9ShxIdZ9",
      "metadata": {
        "id": "w5Ry9ShxIdZ9"
      },
      "outputs": [],
      "source": [
        "plt.plot(cost_history[0], color='orange', label='train')\n",
        "plt.plot(cost_history[1], color='deepskyblue', label='test')\n",
        "plt.title('Train and test error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cost')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oL_xV2i2ONPW",
      "metadata": {
        "id": "oL_xV2i2ONPW"
      },
      "outputs": [],
      "source": [
        "# Inference on training set\n",
        "yhat_train = batched_predict_with_cleanup(train_X, train_y, params, dims)\n",
        "print(f'Training accuracy: {accuracy_score(y_pred=yhat_train, y_true=np.argmax(train_y, axis=0)):.4f}')\n",
        "\n",
        "# Inference on test set\n",
        "yhat_test = batched_predict_with_cleanup(test_X, test_y, params, dims)\n",
        "print(f'Test accuracy: {accuracy_score(y_pred=yhat_test, y_true=np.argmax(test_y, axis=0)):.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}